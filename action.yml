# Mirrors action/download-artifact, taking many of the same parameters, but download from S3 instead of GitHub.
# based on open-turo/actions-s3-artifact
# See: https://github.com/open-turo/actions-s3-artifact/blob/main/download/action.yaml
name: Download AWS S3 Artifact
description: Downloads an artifact from an AWS S3 bucket

# Inputs, outputs and descriptions pulled from actions/download-artifact
inputs:
  name:
    description: Name of the artifact to download. If unspecified, all artifacts for the run are downloaded.
    required: false
  path:
    description: Destination path. Supports basic tilde expansion. Defaults to $GITHUB_WORKSPACE
    required: false
  # TODO: implement pattern
  pattern:
    description: A glob pattern matching the artifacts that should be downloaded. Ignored if name is specified.
    required: false
  # TODO: implement merge-multiple
  merge-multiple:
    description: >
      When multiple artifacts are matched, this changes the behavior of the destination directories.
      If true, the downloaded artifacts will be in the same directory specified by path.
      If false, the downloaded artifacts will be extracted into individual named directories within the specified path.
      required: false
      default: false
  # TODO: implement repository
  repository:
    description: >
      The repository owner and the repository name joined together by "/".
      If github-token is specified, this is the repository that artifacts will be downloaded from.
      required: false
    default: github.repository
  # TODO: implement run-id
  run-id:
    description: >
      The id of the workflow run where the desired download artifact was uploaded from.
      If github-token is specified, this is the run that artifacts will be downloaded from.
      required: false
    default: github.run_id
outputs:
  download-path:
    description: Path of artifact download

runs:
  using: 'composite'
  steps:
    - name: Download artifact
      shell: bash
      run: |
        # check whether AWS credentials are specified and warn if they aren't
        if [[ "${{ env.AWS_ACCESS_KEY_ID }}" == '' || "${{ env.AWS_SECRET_ACCESS_KEY }}" == '' ]]; then
          echo "::warn::AWS_ACCESS_KEY_ID and/or AWS_SECRET_ACCESS_KEY is missing from environment variables."
        fi
      
        # check whether S3_ARTIFACTS_BUCKET is defined
        if [[ "${{ env.S3_ARTIFACTS_BUCKET }}" == '' ]]; then
          echo "::error::S3_ARTIFACTS_BUCKET is missing from environment variables."
          exit 1
        fi

        # Script for URL encoding a string
        # see: https://gist.github.com/cdown/1163649#file-gistfile1-sh
        urlencode() {
            # urlencode <string>

            # save current LC_COLLATE for restoring
            local old_lc_collate=$LC_COLLATE

            # setting LC_COLLATE=C forces a case-sensitive sort, where 'A' comes before 'a'.
            LC_COLLATE=C

            # get the length of first argument
            local length="${#1}"

            local i
            for (( i = 0; i < length; i++ )); do
                # get the ith character from the input
                local c="${1:$i:1}"

                # print character if it doesn't need to be encoded, encode character if it does
                case $c in
                    [a-zA-Z0-9.~_-]) printf '%s' "$c" ;;
                    *) printf '%%%02X' "'$c" ;;
                esac
            done

            # restore original LC_COLLATE
            LC_COLLATE=$old_lc_collate
        }

        # Make sure that the path directory exists
        mkdir -p "${{ inputs.path }}"

        # Ensure we have a unique temporary directory to download to
        TMP_ARTIFACT="$RUNNER_TEMP/download-s3-artifact"
        if [[ "${{ runner.os }}" == "Windows" ]]; then
          # On some windows runners, the path for TMP_ARTIFACT is a mix of windows and unix path (both / and \), which
          # caused errors when un-taring. Converting to unix path resolves this.
          TMP_ARTIFACT=$(cygpath -u "$TMP_ARTIFACT")
        fi
        mkdir -p "$TMP_ARTIFACT"

        # Create a unique directory for this particular action run
        TMPDIR="$(mktemp -d -p "$TMP_ARTIFACT" "download.XXXXXXXX")"
        echo "::debug::Created temporary directory $TMPDIR"

        # Target for download, this can be a single file or a tarball
        TMPFILE="$TMPDIR/download-s3-artifact/artifact.tgz"

        # Get AWS S3 bucket URI and ensure it starts with "s3://"
        S3URI="${{ env.S3_ARTIFACTS_BUCKET }}"
        if [[ "$S3URI" != s3://* ]]; then
          echo "::debug::Adding s3:// to bucket URI"
          S3URI="s3://$S3URI"
        fi

        # S3 URI to download
        # Build key to object in S3 bucket
        REPO="${{ github.repository }}"
        RUN_ID="${{ github.run_id }}"
        KEY="$REPO/$RUN_ID/$(urlencode ${{ inputs.name }}).tgz"
        S3URI="${S3URI%/}/$KEY"
        ARTIFACT_NAME="$(basename $S3URI)"

        # Try to download
        echo '::debug::aws s3 cp "$S3URI" "$TMPFILE"'
        aws s3 cp "$S3URI" "$TMPFILE"
        echo "::debug::File downloaded successfully to $TMPFILE"

        # TODO: What does this do and do we need it?
        # if [[ -n "${{ inputs.strip }}" ]]; then
        #   TAR_CLI_ARGS="--strip-components=${{ inputs.strip }}"
        # fi

        # Downloaded a tarball, extract it
        # TODO: Should we check the path input to make sure it is in the project?
        echo '::debug::tar -xzvf "$TMPFILE" -C "${{ inputs.path }}" $TAR_CLI_ARGS'
        tar -xzvf "$TMPFILE" -C "${{ inputs.path }}" $TAR_CLI_ARGS
        
        if [[ -n "$RUNNER_DEBUG" ]]; then
          echo "::debug::Contents of artifact path"
          echo '$(tree -a "${{ inputs.path }}" 2>&1)'
        fi
          
        # set output
        # TODO: I don't think this output is correct. Need to investigate.
        echo "download-path='${{ inputs.path }}'" >> $GITHUB_OUTPUT

        # clean up temp files
        rm -rf $TMP_ARTIFACT